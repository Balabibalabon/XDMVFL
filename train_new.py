#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
new_train.py

æœ¬ç¨‹å¼ç¢¼æµç¨‹ï¼š
  1. ä¾æ“šè¨­å®šåƒæ•¸å®Œæˆä¸»æ¨¡å‹ï¼ˆVFLï¼‰çš„ clean èˆ‡ poisoning è¨“ç·´ï¼Œ
     ä¸¦åœ¨æœ€å¾Œä¸€å€‹è¨“ç·´ epochï¼ˆä¸‹æ¯’éšæ®µï¼‰ä¸­ç›´æ¥æ”¶é›†ç•¶ä¸‹çš„ concatenated embeddingã€‚
  2. ä¸»æ¨¡å‹è¨“ç·´å®Œç•¢å¾Œï¼Œåˆ©ç”¨æœ€å¾Œä¸€è¼ªæ”¶é›†åˆ°çš„ embedding çµ„æˆé›¢ç·šè³‡æ–™é›† H_trainã€‚
  3. åˆ©ç”¨ H_train é›¢ç·šè¨“ç·´ MAE æ¨¡çµ„ï¼ˆVFLIP é˜²ç¦¦éƒ¨åˆ†ï¼‰ï¼Œè¨“ç·´æ™‚ä½¿ç”¨
     defense_model_structure.py ä¸­çš„ VerticalFLMAELoss é€²è¡Œ loss è¨ˆç®—ã€‚
  4. MAE é›¢ç·šè¨“ç·´å®Œæˆå¾Œï¼Œåˆ‡æ›åˆ° eval æ¨¡å¼ï¼Œä¾›æ¨è«–éšæ®µä½¿ç”¨ã€‚
     
â€» è«–æ–‡ä¸­å°æ–¼ CIFAR10ã€CINIC10ã€NUS-WIDEã€BM è¨­ epochs=20ï¼ŒImagenette ç‚º 50ï¼ˆæœ¬ç¨‹å¼ä¾ args.D æ±ºå®šï¼Œå¯èª¿æ•´ï¼‰ã€‚
â€» æœ¬ç¨‹å¼å‡è¨­å…¶ä»–æ¨¡çµ„ï¼ˆtrain_model_structure.pyã€defense_model_structure.pyã€Attack.pyã€data.py ç­‰ï¼‰å‡å·²æ­£ç¢ºå¯¦ä½œï¼Œ
   ä¸”é˜²ç¦¦æ–¹æ³• args.DM ç‚º "VFLIP" æ™‚å•Ÿå‹•æœ¬é›¢ç·š MAE è¨“ç·´æµç¨‹ã€‚
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import time
import itertools
import os

import train_model_structure as tms
import defense_model_structure as dms
import Attack
import Defense
import data
from abl import ABLLossManager
from NeuralCleanse import NeuralCleanseDefense
from defense_model_structure import CoPurDefense   # NEW
from vflmonitor import FeatureBankBuilder
from defense_model_structure import SHAPMAE_D
from dualgate_utils import compute_cos_robust_stats
import json, time, os

def _now():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.perf_counter()

def _save_json(path, data):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        json.dump(data, f, indent=2)


class PleaseSetCorrectParameter(Exception):
    pass

class PleaseCheckPoisonRatio(Exception):
    pass

def check_poison_ratio(args):
    pr = getattr(args, "poison_ratio", None)
    if args.D in ["CIFAR10","CINIC10"]:
        if (pr is not None) and pr > 0.1:
            raise PleaseCheckPoisonRatio("CIFAR10 and CINIC10 has 10 class, over 10% poison ratio will turn into error")
    elif args.D == "BM":
        if (pr is not None) and pr > 0.45:
            raise PleaseCheckPoisonRatio("BM has 2 class, the minor one is near 47 percent, and over 45 percent poison ratio will turn into error")

def return_batch_size(args):
    if args.D in ["CIFAR10","CINIC10"]:
        return 500
    elif args.D == "BM":
        return 128

# â”€â”€ ä¸»æ¨¡å‹èˆ‡ poisoning è¨“ç·´å‡½å¼ â”€â”€
def poisoning_train(dl_train_set, poisoned_trainloader, trn_x, trn_y, args):
    # åŸºæœ¬åƒæ•¸è¨­å®š
    num_class = args.num_class
    epochs = args.epochs
    start_poison_epoch = args.start_poison_epoch
    attack_method = args.AM
    num_attacker = args.AN
    num_party = args.P

    # æ ¹æ“š args.AL é¸æ“‡æ”»æ“Šè€…
    if args.AL == "F":
        attacker_list = np.arange(num_attacker)
    elif args.AL == "L":
        attacker_list = np.arange(num_party - num_attacker, num_party)
    elif args.AL == "C":
        num_redundant = num_party - num_attacker
        first_redundant = int(num_redundant / 2)
        last_redundant = num_attacker + first_redundant
        attacker_list = list(range(num_party)[first_redundant:last_redundant])
    else:
        attacker_list = np.random.choice(num_party, size=num_attacker, replace=False)
    print("æ”»æ“Šè€…è¨­å®šï¼š", attacker_list)
    not_attacker_list = list(set(range(num_party)) - set(attacker_list))

    # è¨­å®š active èˆ‡ passive party
    active_party = attacker_list[num_attacker - 1] + 1
    print("active party:", active_party)
    other_party = list(range(num_party))
    other_party.remove(active_party)
    passive_party = other_party
    print("passive party:", passive_party)

    # æ”»æ“Šåˆå§‹åŒ–
    if attack_method == "TIFS":
        attack_component = Attack.TIFS_attack(args)
    elif attack_method == "VILLIAN":
        attack_component = Attack.VILLIAN_attack(args)
    elif attack_method == "BadVFL":
        attack_component = Attack.BadVFL_attack(args)
    steal_set, steal_id = attack_component.steal_samples(trn_x, trn_y, args)
    poison_id_set = set(steal_id)

    # å»ºç«‹ä¸»æ¨¡å‹ï¼ˆä»¥ CIFAR10ã€CINIC10 ç‚ºä¾‹ï¼‰
    if args.D in ["CIFAR10", "CINIC10"]:
        model = tms.Model(args)
    elif args.D == "Imagenette":
        model = tms.Model(args)
    elif args.D == "BM":
        feature_slices = data.FEATURE_SLICES
        model = tms.Model(args, feature_slices=feature_slices)
        print("\n[åˆ†é…çµæœ]")
        for pid, roots in enumerate(data.PARTY_ROOTS):
            print(f"Party-{pid}: {roots}")
        print("feature_slices =", feature_slices, "\n")
    
    elif args.D == "NUSWIDE":
        feature_slices = data.FEATURE_SLICES
        model = tms.Model(args, feature_slices=feature_slices)

        print("\n[åˆ†é…çµæœ]")
        for pid, roots in enumerate(data.PARTY_ROOTS):
            print(f"Party-{pid}: {roots}")
        print("feature_slices =", feature_slices, "\n")
    else:
        raise ValueError("Unsupported dataset")

    # è‹¥é˜²ç¦¦æ–¹æ³•ç‚º VFLIPï¼Œåˆå§‹åŒ– MAE æ¨¡çµ„ï¼ˆåƒ…å»ºç«‹ï¼Œä¸åœ¨ä¸»æ¨¡å‹è¨“ç·´æ™‚åŒæ­¥æ›´æ–°ï¼‰
    if args.DM == "VFLIP":
        defense_model = dms.MAE(args, active_party)
        static_class_dict = 0
        all_shapley_outer_dict = {outer_key: {} for outer_key in range(num_class)}
        for k in range(num_class):
            all_shapley_inner_key = list(range(args.P))
            all_shapley_outer_dict[k] = {key: torch.empty(0, device=args.device) for key in all_shapley_inner_key}
    
    elif args.DM == "SHAP_MAE":
        defense_model = dms.SHAPMAE(args, active_party)
        static_class_dict = None
        all_shapley_outer_dict = {outer_key: {} for outer_key in range(num_class)}
        for k in range(num_class):
            all_shapley_inner_key = list(range(args.P))
            all_shapley_outer_dict[k] = {key: torch.empty(0, device=args.device) for key in all_shapley_inner_key}
        into_shapley_count = 0
        id_in_class = []
        steal_id_in_class = []
        
        vec_dict_last_epoch = {}
        for p in range(num_party):
            vec_dict_last_epoch[p] = torch.empty(0, device=args.device)
        y_in_last_epoch = []
    
    elif args.DM == "SHAP_MAE_D":
        defense_model = SHAPMAE_D(args, active_party).to(args.device)
        static_class_dict = None
        all_shapley_outer_dict = {outer_key: {} for outer_key in range(num_class)}
        for k in range(num_class):
            all_shapley_inner_key = list(range(args.P))
            all_shapley_outer_dict[k] = {key: torch.empty(0, device=args.device) for key in all_shapley_inner_key}
        into_shapley_count = 0
        id_in_class = []
        steal_id_in_class = []
        
        vec_dict_last_epoch = {}
        for p in range(num_party):
            vec_dict_last_epoch[p] = torch.empty(0, device=args.device)
        y_in_last_epoch = []
    
    # è‹¥é¸æ“‡ ABLï¼Œå»ºç«‹ç®¡ç†å™¨
    elif args.DM == 'ABL':
        abl_mgr = ABLLossManager(gamma=args.abl_gamma,
                                lowest_percent=args.abl_percent,
                                switch_epoch=args.abl_turn_epoch)
    elif args.DM == "CoPur":
        H_clean_list = [] 
        
    elif args.DM == "VFLMonitor":
        if args.center_loss_weight > 0:
            center_opt = torch.optim.SGD(model.center_loss.parameters(), lr=0.5)
        else:
            print(f"ä½ é¸äº†{args.DM}ï¼Œä½†æ²’æœ‰è¨­å®š center_loss_weight")
            raise PleaseSetCorrectParameter("Please set center_loss_weight")
            
        fb_builder = FeatureBankBuilder(num_parties=args.P,
                                    num_classes=args.num_class)
    else:
        defense_model = None
        static_class_dict = None

    # ä¸»æ¨¡å‹ optimizer è¨­å®š
    if attack_method == "VILLIAN":
        lr_multiplier = 1.2 if args.D == "Imagenette" else 2
        optimizer_clf = optim.Adam([
            {'params': model.top.parameters(), 'lr': 2e-3, 'weight_decay': 1e-5, 'eps': 1e-4},
            {'params': [param for idx in not_attacker_list for param in model.bottommodels[idx].parameters()],
             'lr': 2e-3, 'weight_decay': 1e-5, 'eps': 1e-4},
            {'params': [param for idx in attacker_list for param in model.bottommodels[idx].parameters()],
             'lr': 2e-3 * lr_multiplier, 'weight_decay': 1e-5, 'eps': 1e-4}
        ])
    else:
        optimizer_clf = optim.Adam([
            {'params': model.top.parameters(), 'lr': 2e-3, 'weight_decay': 1e-5, 'eps': 1e-4},
            {'params': model.bottommodels.parameters(), 'lr': 2e-3, 'weight_decay': 1e-5, 'eps': 1e-4}
        ])

    # ç”¨ä»¥æ”¶é›†æœ€å¾Œä¸€è¼ªï¼ˆpoisoning éšæ®µï¼‰embedding çš„åˆ—è¡¨
    last_epoch_embeddings = []
    last_epoch_shapleys = []
    last_epoch_vec_dicts = []
    last_epoch_labels = []
    
    # time è¨“ç·´é–‹å§‹å‰åˆå§‹åŒ–è¨ˆæ™‚å™¨
    timing_train = {"model_s": 0.0, "mae_s": 0.0, "shapley_s": 0.0, "n": 0}
    t0 = _now()

    # â”€â”€ ä¸»æ¨¡å‹è¨“ç·´ â”€â”€
    for epoch in range(epochs):
        if epoch < start_poison_epoch:
            cum_loss = 0.0
            cum_center_loss = 0.0
            cum_acc = 0.0
            tot = 0.0
            for i, (x_in, y_in, _) in enumerate(poisoned_trainloader):
                B = x_in.size(0)
                x_in = x_in.to(args.device)
                main_pred = model(x_in)
                
                classification_loss = model.loss(main_pred, y_in)  # ä»æ˜¯ CE

                # ---------- Center-Loss (per-party, 50-dim) ----------      <<<<<< æ”¹é€™å¡Š
                if args.DM == "VFLMonitor":
                    center_loss = compute_center_loss(model._cached_embed,  # ğŸ‘ˆ helper
                                                    y_in, model.center_loss,
                                                    args.P, args.bottomModel_output_dim)
                    total_loss = classification_loss + args.center_loss_weight * center_loss
                else:
                    center_loss = torch.tensor(0.0, device=classification_loss.device)
                    total_loss  = classification_loss

                # å…©çµ„åƒæ•¸éƒ½è¦ zero_grad & step
                optimizer_clf.zero_grad()
                if args.DM == "VFLMonitor":
                    center_opt.zero_grad()

                total_loss.backward()

                optimizer_clf.step()
                if args.DM == "VFLMonitor":
                    center_opt.step()
                
                cum_loss += classification_loss.item() * B
                if args.DM == "VFLMonitor":
                    cum_center_loss += center_loss.item() * B          # å…ˆåœ¨è¿´åœˆå¤–åˆå§‹åŒ–ç‚º 0
                pred = main_pred.max(1)[1].cpu()
                cum_acc += (pred.eq(y_in)).sum().item()
                tot += B
                
                # time ç´¯è¨ˆè¨“ç·´æ¨£æœ¬æ•¸
                timing_train["n"] += B
                
                if args.DM =="VFLMonitor":
                    with torch.no_grad():
                        # â‘  å– Model forward æ™‚æš«å­˜çš„ concatenated embed
                        B, PD = model._cached_embed.shape
                        d = args.bottomModel_output_dim           # = æ¯äººåµŒå…¥ç¶­åº¦
                        # â‘¡ é‚„åŸæˆ List[Tensor]ï¼Œæ¯ tensor (B, d)
                        party_embeds = [ model._cached_embed[:, i*d:(i+1)*d].detach().cpu()
                                        for i in range(args.P) ]
                        labels_cpu  = y_in.detach().cpu()
                        fb_builder.update(party_embeds, labels_cpu)
                        
            if args.DM == "VFLMonitor":
                print(f"Epoch {epoch+1} (clean): CE={cum_loss/tot:.4f}  "
                    f"Center-loss={cum_center_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
            else:
                print(f"Epoch {epoch+1} (clean): CE={cum_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
            # print("Epoch {} (clean): loss = {:.4f}, acc = {:.4f}".format(epoch+1, cum_loss/tot, cum_acc/tot))
        else:
            # ä»¥ TIFS æ”»æ“Šç‚ºä¾‹ï¼›å…¶ä»–æ”»æ“Šç­–ç•¥å¯åƒç…§åŸé‚è¼¯
            cum_loss = 0.0
            cum_center_loss = 0.0
            cum_acc = 0.0
            tot = 0.0
            is_last_epoch = (epoch == args.epochs - 1)
            
            if attack_method == "TIFS":
                if (epoch - start_poison_epoch) % 10 == 0:
                    attacker_target_vec_dict = attack_component.get_design_vec(model, args, steal_set, attacker_list)
                    model.train()
                for i, (x_in, y_in, id_in) in enumerate(poisoned_trainloader):
                    B = x_in.size(0)
                    x_in = x_in.to(args.device)
                    num_party = args.P
                    
                    x_list = Attack._split_for_parties(x_in, args.P, args.D)
                    vec_dict = {}
                    for p in range(num_party):
                        vec_dict[p] = model.bottommodels[p](x_list[p].clone())
                    # å°æ”»æ“Šè€…é€²è¡Œ poisoningï¼šä»¥ steal_id ç‚ºæ¢ä»¶
                    label_condition = []
                    condition = []
                    for idx in range(B):
                        if id_in[idx] in steal_id:
                            condition.append(idx)
                        if y_in[idx] == args.label:
                            label_condition.append(idx)
                            
                    for attacker in attacker_list:
                        vec_dict[attacker][condition] = torch.tensor(attacker_target_vec_dict[attacker]).to(args.device)
                    vec = torch.cat([vec_dict[p] for p in range(num_party)], dim=1)
                    
                    # è‹¥ç‚ºæœ€å¾Œä¸€è¼ªï¼Œæ”¶é›†ç•¶ä¸‹çš„ embedding vector
                    if epoch == epochs - 1:
                        if args.DM == "VFLIP":
                            last_epoch_embeddings.append(vec.cpu())
                        elif args.DM == "NEURAL_CLEANSE":
                            last_epoch_embeddings.append(vec.cpu())
                            last_epoch_labels.extend(y_in.cpu())
                        elif args.DM == "SHAP_MAE":
                            #time
                            ts0 = _now()
                            last_epoch_embeddings.append(vec.cpu())
                            for index in range(len(condition)):
                                condition[index] = condition[index] + (args.batch_size * into_shapley_count)
                            for index in range(len(label_condition)):
                                label_condition[index] = label_condition[index] + (args.batch_size * into_shapley_count)
                            steal_id_in_class.extend(condition)
                            id_in_class.extend(label_condition)
                            
                            for p in range(num_party):
                                vec_dict_last_epoch[p] = torch.cat((vec_dict_last_epoch[p], vec_dict[p]), dim=0)
                            y_in_last_epoch.extend(y_in)
                            
                            '''
                            å»ºä¸€å€‹ dict è’é›†æ¯å€‹é¡åˆ¥æ¯å€‹åƒèˆ‡è€…çš„è²¢ç»åº¦åˆ†å¸ƒï¼Œdo_shapley è¨ˆç®—å®Œä¹‹å¾Œå›å‚³æ›´æ–°æ­¤ dict
                            '''
                            with torch.no_grad():
                                vec_dict_shap = {}
                                for p in range(num_party):
                                    vec_dict_shap[p] = model.bottommodels[p](x_list[p].clone().detach())
                                vec_shap = torch.cat([vec_dict_shap[p] for p in range(num_party)], dim=1)
                                returned_shapley_outer_dict = Defense.do_shapley(model, vec_shap, y_in, args)
                            for k in range(num_class):
                                for p in range(num_party): 
                                    all_shapley_outer_dict[k][p] = torch.cat(
                                        (all_shapley_outer_dict[k][p], returned_shapley_outer_dict[k][p]), dim=0)
                            print(f"\rç¾åœ¨æ­£åœ¨è¨ˆç®—è¨“ç·´æ™‚ shapley valueï¼Œé€²åº¦ç‚ºï¼š{round(len(all_shapley_outer_dict[k][p])/(len(trn_y)/num_class),2)*100}%"," "*20,end="")
                            into_shapley_count+=1
                            #time
                            ts1 = _now()
                            timing_train["shapley_s"] += (ts1 - ts0)
                            
                        elif args.DM == "SHAP_MAE_D":
                            last_epoch_embeddings.append(vec.cpu())
                            for index in range(len(condition)):
                                condition[index] = condition[index] + (args.batch_size * into_shapley_count)
                            for index in range(len(label_condition)):
                                label_condition[index] = label_condition[index] + (args.batch_size * into_shapley_count)
                            steal_id_in_class.extend(condition)
                            id_in_class.extend(label_condition)
                            
                            for p in range(num_party):
                                vec_dict_last_epoch[p] = torch.cat((vec_dict_last_epoch[p], vec_dict[p]), dim=0)
                            y_in_last_epoch.extend(y_in)
                            
                            '''
                            å»ºä¸€å€‹ dict è’é›†æ¯å€‹é¡åˆ¥æ¯å€‹åƒèˆ‡è€…çš„è²¢ç»åº¦åˆ†å¸ƒï¼Œdo_shapley è¨ˆç®—å®Œä¹‹å¾Œå›å‚³æ›´æ–°æ­¤ dict
                            '''
                            with torch.no_grad():
                                vec_dict_shap = {}
                                for p in range(num_party):
                                    vec_dict_shap[p] = model.bottommodels[p](x_list[p].clone().detach())
                                vec_shap = torch.cat([vec_dict_shap[p] for p in range(num_party)], dim=1)
                                returned_shapley_outer_dict = Defense.do_shapley(model, vec_shap, y_in, args)
                            for k in range(num_class):
                                for p in range(num_party): 
                                    all_shapley_outer_dict[k][p] = torch.cat(
                                        (all_shapley_outer_dict[k][p], returned_shapley_outer_dict[k][p]), dim=0)
                            print(f"\rç¾åœ¨æ­£åœ¨è¨ˆç®—è¨“ç·´æ™‚ shapley valueï¼Œé€²åº¦ç‚ºï¼š{round(len(all_shapley_outer_dict[k][p])/(len(trn_y)/num_class),2)*100}%"," "*20,end="")
                            into_shapley_count+=1
                        
                        elif args.DM == "CoPur":
                            # batch_id éœ€éš¨ DataLoader å›å‚³ï¼›è‹¥ç„¡è«‹ä¿®æ”¹ Dataset ä»¤å…¶å›å‚³ idx
                            batch_id = id_in.to('cpu')          # ç¢ºä¿èƒ½è½‰æˆ python int
                            clean_mask = torch.tensor(
                                [ (int(i) not in poison_id_set) for i in batch_id ],
                                dtype=torch.bool, device=vec.device
                            )
                            if clean_mask.any():                   # è‡³å°‘æœ‰ä¸€ç­†ä¹¾æ·¨æ‰ append
                                H_clean_list.append(vec[clean_mask].detach().cpu())
                    # ---------------------------------------------------------
                    #  å‰å‘å‚³æ’­
                    # ---------------------------------------------------------
                    main_pred = model.top(vec)                    # logits, shape (B, num_class)
                    
                    # ---------- ç¢ºä¿ y_in åœ¨åŒä¸€å¼µ GPU ----------
                    y_in = y_in.to(main_pred.device)

                    # ---------------------------------------------------------
                    #  (1) è¨ˆç®—ã€Œæ¯ç­†ã€äº¤å‰ç†µ (reduction='none')
                    #      è‹¥ model.loss åªæœ‰ mean ç‰ˆæœ¬ï¼Œå°±æ”¹ç”¨ F.cross_entropyã€‚
                    # ---------------------------------------------------------
                    loss_vec = F.cross_entropy(main_pred, y_in, reduction='none')   # shape (B,)

                    # ---------------------------------------------------------
                    #  (2) ä¾é¸æ“‡çš„é˜²ç¦¦æ–¹æ³•ç”¢ç”Ÿ total_loss
                    # ---------------------------------------------------------
                    if args.DM == 'ABL':
                        # 2â€‘a æ”¶é›†æ¨£æœ¬ lossï¼ˆåªåœ¨ LGA éšæ®µçœŸæ­£å­˜èµ·ä¾†ï¼‰
                        abl_mgr.collect(id_in, loss_vec.detach(), epoch)

                        # 2â€‘b é€éå…©éšæ®µå…¬å¼å¾—åˆ°ä¸€å€‹ scalar total_loss
                        total_loss = abl_mgr.total_loss(loss_vec, id_in, epoch)

                        # 2â€‘c ç‚ºäº†æ—¥èªŒçµ±è¨ˆï¼Œä»ä¿ç•™ mean lossï¼ˆæ–¹ä¾¿èˆ‡ baseline æ¯”ï¼‰
                        display_loss = loss_vec.mean().item()
                    else:
                        # é ABL â†’ èˆ‡åŸå…ˆè¡Œç‚ºç›¸åŒ
                        total_loss   = loss_vec.mean()
                        display_loss = total_loss.item()
                        # -------- Center-Loss (per-party) --------                <<<<<< æ”¹é€™å¡Š
                        if args.DM == "VFLMonitor":
                            center_loss = compute_center_loss(vec, y_in, model.center_loss,
                                                            args.P, args.bottomModel_output_dim)
                            total_loss = total_loss + args.center_loss_weight * center_loss
                        else:
                            center_loss = torch.tensor(0.0, device=total_loss.device)
                        # -----------------------------------------
                    
                    # -------- Back-prop --------(åŠ å…¥ VFLMonitor å¾Œç‰ˆæœ¬)
                    optimizer_clf.zero_grad()
                    if args.DM == "VFLMonitor":
                        center_opt.zero_grad()

                    total_loss.backward()

                    optimizer_clf.step()
                    if args.DM == "VFLMonitor":
                        center_opt.step()
                    
                    # -------- Metrics --------(åŠ å…¥ VFLMonitor å¾Œç‰ˆæœ¬)
                    cum_loss += display_loss * B
                    if args.DM == "VFLMonitor":
                        cum_center_loss += center_loss.item() * B

                    '''
                    åŸå…ˆè¨ˆç®—æ–¹å¼
                    pred = main_pred.max(1)[1].cpu()
                    cum_acc += (pred.eq(y_in)).sum().item()
                    '''
    
                    pred = main_pred.argmax(1)              # GPU tensor
                    cum_acc += (pred == y_in).sum().item()
                    tot      += B
                    #time
                    timing_train["n"] += B  # B = batch_size
                    
                # ---------- epoch çµå°¾ ----------
                if args.DM == 'ABL':
                    abl_mgr.finalize_LGA(epoch)
                    
                if args.DM == "VFLMonitor":
                    print(f"Epoch {epoch+1} (poisoning): CE={cum_loss/tot:.4f}  "
                        f"Center-loss={cum_center_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
                else:
                    print(f"Epoch {epoch+1} (poisoning): CE={cum_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
                # print("Epoch {} (poisoning): loss = {:.4f}, acc = {:.4f}".format(epoch+1, cum_loss/tot, cum_acc/tot))
            
            elif attack_method == "VILLIAN":
                attacker_target_vec_dict = attack_component.get_design_vec(model, args, poisoned_trainloader, attacker_list, epoch)
                model.train()
                    
                for i, (x_in, y_in, id_in) in enumerate(poisoned_trainloader):
                    B = x_in.size()[0]
                    x_in = x_in.to(args.device)
                    # å°‡åœ–ç‰‡ x å¹³å‡åˆ†æˆ num_party ä»½
                    # x_list = Attack._split_for_parties()
                    x_list = Attack._split_for_parties(x_in, args.P, args.D)
                    
                    vec_dict = {}
                    for p in range(num_party):
                        # vec_dict[p] = model.bottommodels[p](x_list[p])
                        vec_dict[p] = model.bottommodels[p](x_list[p].clone())
                    
                    label_condition = []
                    condition = []
                    for idx in range(B):
                        if id_in[idx] in steal_id:
                            condition.append(idx)
                        if y_in[idx] == args.label:
                            label_condition.append(idx)
                    
                    for attacker in attacker_list:
                        
                        vec_dict[attacker][condition] = (
                                    vec_dict[attacker][condition] +
                                    attacker_target_vec_dict[attacker].clone().detach().cuda()
                                    # torch.tensor(attacker_target_vec_dict[attacker], device=args.device)
                                )
                        
                    vec = torch.cat([vec_dict[p] for p in range(num_party)], dim=1)
                    
                    # è‹¥ç‚ºæœ€å¾Œä¸€è¼ªï¼Œæ”¶é›†ç•¶ä¸‹çš„ embedding vector
                    if epoch == epochs - 1:
                        if args.DM == "VFLIP":
                            last_epoch_embeddings.append(vec.cpu())
                            
                        elif args.DM == "SHAP_MAE":
                            #time
                            ts0 = _now()
                            last_epoch_embeddings.append(vec.cpu())
                            for index in range(len(condition)):
                                condition[index] = condition[index] + (args.batch_size * into_shapley_count)
                            for index in range(len(label_condition)):
                                label_condition[index] = label_condition[index] + (args.batch_size * into_shapley_count)
                            steal_id_in_class.extend(condition)
                            id_in_class.extend(label_condition)
                            
                            for p in range(num_party):
                                vec_dict_last_epoch[p] = torch.cat((vec_dict_last_epoch[p], vec_dict[p]), dim=0)
                            y_in_last_epoch.extend(y_in)
                            
                            '''
                            å»ºä¸€å€‹ dict è’é›†æ¯å€‹é¡åˆ¥æ¯å€‹åƒèˆ‡è€…çš„è²¢ç»åº¦åˆ†å¸ƒï¼Œdo_shapley è¨ˆç®—å®Œä¹‹å¾Œå›å‚³æ›´æ–°æ­¤ dict
                            '''
                            with torch.no_grad():
                                vec_dict_shap = {}
                                for p in range(num_party):
                                    vec_dict_shap[p] = model.bottommodels[p](x_list[p].clone().detach())
                                vec_shap = torch.cat([vec_dict_shap[p] for p in range(num_party)], dim=1)
                                returned_shapley_outer_dict = Defense.do_shapley(model, vec_shap, y_in, args)
                            for k in range(num_class):
                                for p in range(num_party): 
                                    all_shapley_outer_dict[k][p] = torch.cat(
                                        (all_shapley_outer_dict[k][p], returned_shapley_outer_dict[k][p]), dim=0)
                            print(f"\rç¾åœ¨æ­£åœ¨è¨ˆç®—è¨“ç·´æ™‚ shapley valueï¼Œé€²åº¦ç‚ºï¼š{round(len(all_shapley_outer_dict[k][p])/(len(trn_y)/num_class),2)*100}%"," "*20,end="")
                            into_shapley_count+=1
                            #time
                            ts1 = _now()
                            timing_train["shapley_s"] += (ts1 - ts0)
                        
                        elif args.DM == "SHAP_MAE_D":
                            
                            last_epoch_embeddings.append(vec.cpu())
                            for index in range(len(condition)):
                                condition[index] = condition[index] + (args.batch_size * into_shapley_count)
                            for index in range(len(label_condition)):
                                label_condition[index] = label_condition[index] + (args.batch_size * into_shapley_count)
                            steal_id_in_class.extend(condition)
                            id_in_class.extend(label_condition)
                            
                            for p in range(num_party):
                                vec_dict_last_epoch[p] = torch.cat((vec_dict_last_epoch[p], vec_dict[p]), dim=0)
                            y_in_last_epoch.extend(y_in)
                            
                            '''
                            å»ºä¸€å€‹ dict è’é›†æ¯å€‹é¡åˆ¥æ¯å€‹åƒèˆ‡è€…çš„è²¢ç»åº¦åˆ†å¸ƒï¼Œdo_shapley è¨ˆç®—å®Œä¹‹å¾Œå›å‚³æ›´æ–°æ­¤ dict
                            '''
                            with torch.no_grad():
                                vec_dict_shap = {}
                                for p in range(num_party):
                                    vec_dict_shap[p] = model.bottommodels[p](x_list[p].clone().detach())
                                vec_shap = torch.cat([vec_dict_shap[p] for p in range(num_party)], dim=1)
                                returned_shapley_outer_dict = Defense.do_shapley(model, vec_shap, y_in, args)
                            for k in range(num_class):
                                for p in range(num_party): 
                                    all_shapley_outer_dict[k][p] = torch.cat(
                                        (all_shapley_outer_dict[k][p], returned_shapley_outer_dict[k][p]), dim=0)
                            print(f"\rç¾åœ¨æ­£åœ¨è¨ˆç®—è¨“ç·´æ™‚ shapley valueï¼Œé€²åº¦ç‚ºï¼š{round(len(all_shapley_outer_dict[k][p])/(len(trn_y)/num_class),2)*100}%"," "*20,end="")
                            into_shapley_count+=1

                        elif args.DM =="NEURAL_CLEANSE":
                            last_epoch_embeddings.append(vec.cpu())
                            last_epoch_labels.extend(y_in.cpu())
                        elif args.DM == "CoPur":
                            # batch_id éœ€éš¨ DataLoader å›å‚³ï¼›è‹¥ç„¡è«‹ä¿®æ”¹ Dataset ä»¤å…¶å›å‚³ idx
                            batch_id = id_in.to('cpu')          # ç¢ºä¿èƒ½è½‰æˆ python int
                            clean_mask = torch.tensor(
                                [ (int(i) not in poison_id_set) for i in batch_id ],
                                dtype=torch.bool, device=vec.device
                            )
                            if clean_mask.any():                   # è‡³å°‘æœ‰ä¸€ç­†ä¹¾æ·¨æ‰ append
                                H_clean_list.append(vec[clean_mask].detach().cpu())
            
                    # ---------------------------------------------------------
                    #  å‰å‘å‚³æ’­
                    # ---------------------------------------------------------
                    main_pred = model.top(vec)                    # logits, shape (B, num_class)
                    
                    # ---------- ç¢ºä¿ y_in åœ¨åŒä¸€å¼µ GPU ----------
                    y_in = y_in.to(main_pred.device)

                    # ---------------------------------------------------------
                    #  (1) è¨ˆç®—ã€Œæ¯ç­†ã€äº¤å‰ç†µ (reduction='none')
                    #      è‹¥ model.loss åªæœ‰ mean ç‰ˆæœ¬ï¼Œå°±æ”¹ç”¨ F.cross_entropyã€‚
                    # ---------------------------------------------------------
                    loss_vec = F.cross_entropy(main_pred, y_in, reduction='none')   # shape (B,)

                    # ---------------------------------------------------------
                    #  (2) ä¾é¸æ“‡çš„é˜²ç¦¦æ–¹æ³•ç”¢ç”Ÿ total_loss
                    # ---------------------------------------------------------
                    if args.DM == 'ABL':
                        # 2â€‘a æ”¶é›†æ¨£æœ¬ lossï¼ˆåªåœ¨ LGA éšæ®µçœŸæ­£å­˜èµ·ä¾†ï¼‰
                        abl_mgr.collect(id_in, loss_vec.detach(), epoch)

                        # 2â€‘b é€éå…©éšæ®µå…¬å¼å¾—åˆ°ä¸€å€‹ scalar total_loss
                        total_loss = abl_mgr.total_loss(loss_vec, id_in, epoch)

                        # 2â€‘c ç‚ºäº†æ—¥èªŒçµ±è¨ˆï¼Œä»ä¿ç•™ mean lossï¼ˆæ–¹ä¾¿èˆ‡ baseline æ¯”ï¼‰
                        display_loss = loss_vec.mean().item()
                    else:
                        # é ABL â†’ èˆ‡åŸå…ˆè¡Œç‚ºç›¸åŒ
                        total_loss   = loss_vec.mean()
                        display_loss = total_loss.item()
                        # -------- Center-Loss (per-party) --------                <<<<<< æ”¹é€™å¡Š
                        if args.DM == "VFLMonitor":
                            center_loss = compute_center_loss(vec, y_in, model.center_loss,
                                                            args.P, args.bottomModel_output_dim)
                            total_loss = total_loss + args.center_loss_weight * center_loss
                        else:
                            center_loss = torch.tensor(0.0, device=total_loss.device)
                        # -----------------------------------------
                    
                    # -------- Back-prop --------(åŠ å…¥ VFLMonitor å¾Œç‰ˆæœ¬)
                    optimizer_clf.zero_grad()
                    if args.DM == "VFLMonitor":
                        center_opt.zero_grad()

                    total_loss.backward()

                    optimizer_clf.step()
                    if args.DM == "VFLMonitor":
                        center_opt.step()
                    
                    # -------- Metrics --------(åŠ å…¥ VFLMonitor å¾Œç‰ˆæœ¬)
                    cum_loss += display_loss * B
                    if args.DM == "VFLMonitor":
                        cum_center_loss += center_loss.item() * B    
                    pred = main_pred.argmax(1)              # GPU tensor
                    cum_acc += (pred == y_in).sum().item()
                    tot      += B
                    timing_train["n"] += B  # B = batch_size
                    
                # ---------- epoch çµå°¾ ----------
                if args.DM == 'ABL':
                    abl_mgr.finalize_LGA(epoch)
                    
                if args.DM == "VFLMonitor":
                    print(f"Epoch {epoch+1} (poisoning): CE={cum_loss/tot:.4f}  "
                        f"Center-loss={cum_center_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
                else:
                    print(f"Epoch {epoch+1} (poisoning): CE={cum_loss/tot:.4f}  ACC={cum_acc/tot:.4f}")
                # print("Epoch {} (poisoning): loss = {:.4f}, acc = {:.4f}".format(epoch+1, cum_loss/tot, cum_acc/tot))
                
        if (args.DM in ["SHAP","SHAP_MAE","SHAP_MAE_CVPR","SHAP_MAE_D"]) and (epoch == epochs-1):
            print("shapley è¨ˆç®—å®Œç•¢ï¼Œæ­£åœ¨ç¹ªåœ–...")
            static_class_dict = Defense.calculateAndDraw_distribution_of_shapley_value(all_shapley_outer_dict, args, draw=False, id_in_class=id_in_class, steal_id_in_class=steal_id_in_class)
            if args.DM == "SHAP_MAE_D":
                C, P = args.num_class, args.P
                med_mat = torch.zeros(C, P, device=args.device)
                mad_mat = torch.ones (C, P, device=args.device)

                for c in range(C):
                    for p in range(P):
                        med_mat[c, p] = static_class_dict[c][p]["median"]
                        mad_mat[c, p] = static_class_dict[c][p]["mad"]

                defense_model.shap_med.copy_(med_mat)
                defense_model.shap_mad.copy_(mad_mat)
    print("ä¸»æ¨¡å‹è¨“ç·´çµæŸï¼")
    
    # â”€â”€ é›¢ç·š MAE è¨“ç·´ â”€â”€
    if args.DM == "VFLIP":
        # å°‡æœ€å¾Œä¸€è¼ªæ”¶é›†åˆ°çš„ embedding vector çµ„æˆ H_train
        if len(last_epoch_embeddings) == 0:
            raise ValueError("æœªæ”¶é›†åˆ°æœ€å¾Œä¸€è¼ªçš„ embeddingï¼Œè«‹ç¢ºèª poisoning éšæ®µæ­£ç¢ºåŸ·è¡Œã€‚")
        Htrain = torch.cat(last_epoch_embeddings, dim=0)
        print("æ”¶é›†åˆ°çš„ embedding æ•¸é‡ï¼ˆæœ€å¾Œä¸€è¼ªï¼‰ï¼š", Htrain.shape[0])
        
        print("é–‹å§‹é›¢ç·šè¨“ç·´ MAE ...")
        #time
        tm0 = _now()
        # åˆ©ç”¨ defense_model ä¸­ VerticalFLMAELoss é€²è¡Œ loss è¨ˆç®—
        defense_model = train_MAE_offline(defense_model, Htrain, args)
        collect_thresholds_into_mae(
                defense_model = defense_model,
                Htrain_tensor = Htrain,
                args          = args,
                save_path     = "./vflip_thr.pt")
        
        '''
        test
        '''
        print("âŸ¹ é–€æª» dict é•·åº¦ï¼š", len(defense_model.adaptive_thresholds))   # æ‡‰è©² = args.P
        print("âŸ¹ å‰ 5 å€‹é–€æª»å€¼ï¼š", list(defense_model.adaptive_thresholds.items())[:5])
        print("âŸ¹ å…¶ä¸­æœ€å¤§å€¼ / æœ€å°å€¼ï¼š",
            max(defense_model.adaptive_thresholds.values()),
            min(defense_model.adaptive_thresholds.values()))

        print("MAE é›¢ç·šè¨“ç·´å®Œæˆã€‚")
        #time
        tm1 = _now()
        timing_train["mae_s"] += (tm1 - tm0)
        
    # â”€â”€ é›¢ç·š SHAP_MAE è¨“ç·´ â”€â”€
    elif args.DM == "SHAP_MAE":
        # å°‡æœ€å¾Œä¸€è¼ªæ”¶é›†åˆ°çš„ embedding vector çµ„æˆ H_train
        if len(last_epoch_embeddings) == 0:
            raise ValueError("æœªæ”¶é›†åˆ°æœ€å¾Œä¸€è¼ªçš„ embeddingï¼Œè«‹ç¢ºèª poisoning éšæ®µæ­£ç¢ºåŸ·è¡Œã€‚")
        Htrain = torch.cat(last_epoch_embeddings, dim=0)
        print("æ”¶é›†åˆ°çš„ embedding æ•¸é‡ï¼ˆæœ€å¾Œä¸€è¼ªï¼‰ï¼š", Htrain.shape[0])
        
        print("é–‹å§‹é›¢ç·šè¨“ç·´ SHAP_MAE ...")
        #time
        tm0 = _now()
        # åˆ©ç”¨ defense_model ä¸­ VerticalFLMAELoss é€²è¡Œ loss è¨ˆç®—
        defense_model = train_SHAP_MAE_offline(defense_model, Htrain, args)
        print("SHAP_MAE é›¢ç·šè¨“ç·´å®Œæˆã€‚")
        #time
        tm1 = _now()
        timing_train["mae_s"] += (tm1 - tm0)
    
    # â”€â”€ é›¢ç·š SHAP_MAE_D è¨“ç·´ â”€â”€
    elif args.DM == "SHAP_MAE_D":
        H_train = torch.cat(last_epoch_embeddings, dim=0).to(args.device)
        Y_train = torch.tensor(y_in_last_epoch , device=args.device)
        defense_model = train_SHAP_MAE_D_offline(defense_model, H_train, Y_train, args)
        # --- Step 1â€†Aï¼šæŠŠé–€æª»è½‰æˆ tensor ---
        P = args.P
        num_cls = args.num_class
        device = args.device

        shap_up_tbl   = torch.zeros(num_cls, P, device=device)
        shap_down_tbl = torch.zeros_like(shap_up_tbl)

        for c in range(num_cls):
            for p in range(P):
                shap_up_tbl[c, p]   = static_class_dict[c][p]["up_threshold"]
                shap_down_tbl[c, p] = static_class_dict[c][p]["down_threshold"]

        # --- Step 1â€†Bï¼šç¶åˆ° defense_modelï¼ˆæ–¹ä¾¿å­˜å– & å­˜æª”ï¼‰ ---
        defense_model.shap_up_tbl   = shap_up_tbl
        defense_model.shap_down_tbl = shap_down_tbl

    # â”€â”€ é›¢ç·š NEURAL_CLEANSE è¨“ç·´ â”€â”€
    elif args.DM == "NEURAL_CLEANSE":
        # â‘  ä¸²æˆ (N,D) tensor + label
        H_train_tensor = torch.cat(last_epoch_embeddings, dim=0).to(args.device)
        # Y_train_tensor = torch.tensor(y_in_last_epoch, device=args.device)
        Y_train_tensor = torch.tensor(last_epoch_labels, device=args.device)
        
        print("é–‹å§‹é›¢ç·šæœé›† NEURAL_CLEANSE æ•¸æ“š...")
        # â‘¡ åå‘å·¥ç¨‹
        nc_def = NeuralCleanseDefense(args, model.top, args.device)
        nc_def.fit(H_train_tensor, Y_train_tensor)
        # â‘¢ æš«å­˜çµ¦ evaluation ç”¨
        defense_model = nc_def
        print("NEURAL_CLEANSE é›¢ç·šæœé›†å®Œæˆã€‚")
        all_shapley_outer_dict = None
        
    # â”€â”€ é›¢ç·š NEURAL_CLEANSE è¨“ç·´ â”€â”€ 
    elif args.DM == "CoPur":
        H_clean_tensor = torch.cat(H_clean_list, dim=0)   # (N_clean , P*D)
        copur = CoPurDefense(args=args)
        copur.train_autoencoder(H_clean_tensor)
        defense_model = copur
        all_shapley_outer_dict = None
    
    elif args.DM == "VFLMonitor":
        # ----------------  å°‡ bank å¹³å‡ã€åºåˆ—åŒ–  ----------------
        from pathlib import Path
        out_dir = Path("VFLMonitor")
        out_dir.mkdir(exist_ok=True)
        bank_path = os.path.join(out_dir, 'vflmonitor_bank.pt')
        fb_builder.save(bank_path)     # å…§éƒ¨è‡ªå‹• finalizeâ†’å¹³å‡â†’torch.save
        print(f"[VFLMonitor] Feature bank saved to {bank_path}")
        bank = torch.load(bank_path)
        print(bank[0][0].shape)   # æ‡‰é¡¯ç¤º e.g. torch.Size([1205, 256])
        all_shapley_outer_dict = None 
        # -------------------------------------------------------

    else:
        all_shapley_outer_dict = None        # ç¢ºä¿è‡³å°‘çµ¦ä¸€å€‹å€¼
        copur = None
    
    #time ç´€éŒ„è¨“ç·´æ™‚é•·
    t1 = _now()
    timing_train["model_s"] += (t1 - t0)
    
    defense_output = {
        "defense_model": defense_model if args.DM in ["VFLIP","SHAP_MAE","SHAP_MAE_D","NEURAL_CLEANSE","CoPur"] else None,
        "defense_models": None,
        "static_class_dict": static_class_dict if args.DM in ["SHAP_MAE","SHAP_MAE_D"] else None,
        "vec_dict_last_epoch": vec_dict_last_epoch if args.DM in ["SHAP_MAE","SHAP_MAE_D"] else None,
        "y_in_last_epoch": y_in_last_epoch if args.DM in ["SHAP_MAE","SHAP_MAE_D"] else None
    }
    
    n = timing_train["n"]
    _save_json(f"timings/train_overall_{args.DM}_P{args.P}.json", {
        "samples": n,
        "total_model_s": timing_train["model_s"],
        "total_mae_s": timing_train["mae_s"],
        "total_shapley_s": timing_train["shapley_s"],
        "avg_model_ms_per_sample": (timing_train["model_s"] / n) * 1000,
        "avg_mae_ms_per_sample": (timing_train["mae_s"] / n) * 1000,
        "avg_shapley_ms_per_sample": (timing_train["shapley_s"] / n) * 1000
    })
    
    return model, attacker_list, attacker_target_vec_dict, active_party, passive_party, defense_output, all_shapley_outer_dict

# â”€â”€ é›¢ç·š MAE è¨“ç·´å‡½å¼ â”€â”€
def train_MAE_offline(defense_model, Htrain_tensor, args, epochs_MAE=None, batch_size_MAE=500):
    """
    é›¢ç·šè¨“ç·´ MAE æ¨¡çµ„
    Htrain_tensor: [N_total, P * bottomModel_output_dim]
    è«–æ–‡ä¸­å°æ–¼ CIFAR10ã€CINIC10ã€NUS-WIDEã€BM è¨­ epochs=20ï¼ŒImagenette è¨­ epochs=50
    """
    device = args.device
    defense_model.to(device)
    defense_model.train_mode = True
    batch_size_MAE = args.batch_size
    
    mu = Htrain_tensor.mean(dim=0, keepdim=True)   # shape = [1, D]
    std = Htrain_tensor.std(dim=0, keepdim=True) + 1e-5  # é¿å…é™¤åˆ° 0
    defense_model.set_Normalization(mu,std)
    
    if epochs_MAE is None:
        if args.D == "Imagenette":
            epochs_MAE = 50
        else:
            epochs_MAE = 20
    # ä½¿ç”¨ Adam å„ªåŒ–å™¨ï¼Œæ­¤è™•å­¸ç¿’ç‡ä»¥è«–æ–‡ N-1 to 1 ç­–ç•¥ lr=0.01 ç‚ºä¾‹
    optimizer = optim.Adam(defense_model.parameters(), lr=0.1, weight_decay=1e-4, eps=1e-4) #TIFS 8 äººå˜—è©¦
    # optimizer = optim.Adam(defense_model.parameters(), lr=0.05, weight_decay=1e-4, eps=1e-4)
    dataset_MAE = torch.utils.data.TensorDataset(Htrain_tensor)
    dataloader_MAE = torch.utils.data.DataLoader(dataset_MAE, batch_size=batch_size_MAE, shuffle=True)

    # åˆå§‹åŒ– VerticalFLMAELossï¼ˆä¾†è‡ª defense_model_structure.pyï¼‰
    # vertical_loss_fn = dms.VerticalFLMAELoss()
    for e in range(epochs_MAE):
        epoch_loss = 0.0
        for batch in dataloader_MAE:
            batch_x = batch[0].to(device).detach()  # detach ä»¥åˆ‡æ–·ä¹‹å‰çš„åœ–
            mae_input = batch_x.clone()  # clone å‡ºä¸€ä»½ç¨ç«‹çš„å¼µé‡
            MAE_output = defense_model(mae_input)
            MAE_loss = defense_model.loss(MAE_output)
            # loss = vertical_loss_fn(model_output)
            optimizer.zero_grad()
            MAE_loss.backward()  # é€™è£¡ä¸éœ€è¦ retain_graph=True
            optimizer.step()
            epoch_loss += MAE_loss.item()
        print("[MAE Offline] Epoch {}/{}: loss = {:.4f}".format(e+1, epochs_MAE, epoch_loss/len(dataloader_MAE)))
    defense_model.train_mode = False
    # defense_model.set_adaptive_threshold(Htrain_tensor)
    return defense_model

@torch.no_grad()
def collect_thresholds_into_mae(
        defense_model: dms.MAE,
        Htrain_tensor: torch.Tensor,
        args,
        save_path: str | None = None,   # â† None è¡¨ç¤ºä¸è½æª”
        subsample_ratio: float = 0.7,
        rho: float = 2.0,
        batch_size: int = 256):

    device, P, D_perP = args.device, args.P, args.bottomModel_output_dim
    defense_model.eval()

    # ======= 1. optional subsample =======
    K_all = Htrain_tensor.size(0)
    if subsample_ratio < 1.0:
        idx = torch.randperm(K_all, device=Htrain_tensor.device)[: int(K_all*subsample_ratio)]
        H_use = Htrain_tensor[idx]
    else:
        H_use = Htrain_tensor

    loader = torch.utils.data.DataLoader(
                torch.utils.data.TensorDataset(H_use),
                batch_size=batch_size, shuffle=False)

    scores = [ [] for _ in range(P) ]
    eyeP   = torch.eye(P, device=device)
    mu, std = defense_model.mu, defense_model.std
    norm_en = defense_model.normalize

    for (xb,) in loader:
        xb = xb.to(device)
        x_in = (xb - mu)/std if norm_en else xb
        x_in = x_in.view(x_in.size(0), -1)

        for j in range(P):
            mj = eyeP[j:j+1].repeat(x_in.size(0),1).repeat_interleave(D_perP, dim=1)
            recon = defense_model.decoder(defense_model.encoder(x_in * mj))
            for i in range(P):
                if i == j: continue
                st, ed = i*D_perP, (i+1)*D_perP
                diff = recon[:,st:ed] - x_in[:,st:ed]
                scores[i].append(diff.norm(2, dim=1).cpu())

    adaptive = {}
    if P ==2:
        rho = 2.0
    elif P==4:
        rho = 1.2
    else: 
        rho = 1.0
    for i in range(P):
        s = torch.cat(scores[i])
        mu_i, sigma_i = s.mean().item(), s.std(unbiased=False).item()
        adaptive[i]   = mu_i + rho * sigma_i

    # ======= 2. å¯«å…¥ MAE ç‰©ä»¶ =======
    defense_model.adaptive_thresholds = adaptive
    print("[VFLIP] thresholds stored inside MAE.adaptive_thresholds")

    # ======= 3. å¯é¸ï¼šå¦å­˜æª” =======
    if save_path is not None:
        torch.save(adaptive, save_path)
        print(f"           â€¦ and serialized to {save_path}")

    return adaptive

def train_SHAP_MAE_offline(defense_model, Htrain_tensor, args, epochs_MAE=None, batch_size_MAE=500):
    """
    é›¢ç·šè¨“ç·´ MAE æ¨¡çµ„
    Htrain_tensor: [N_total, P * bottomModel_output_dim]
    è«–æ–‡ä¸­å°æ–¼ CIFAR10ã€CINIC10ã€NUS-WIDEã€BM è¨­ epochs=20ï¼ŒImagenette è¨­ epochs=50
    """
    device = args.device
    defense_model.to(device)
    defense_model.train_mode = True
    batch_size_MAE = args.batch_size
    
    mu = Htrain_tensor.mean(dim=0, keepdim=True)   # shape = [1, D]
    std = Htrain_tensor.std(dim=0, keepdim=True) + 1e-5  # é¿å…é™¤åˆ° 0
    defense_model.set_Normalization(mu,std)
    
    if epochs_MAE is None:
        if args.D == "Imagenette":
            epochs_MAE = 50
        else:
            epochs_MAE = 20
    # ä½¿ç”¨ Adam å„ªåŒ–å™¨ï¼Œæ­¤è™•å­¸ç¿’ç‡ä»¥è«–æ–‡ N-1 to 1 ç­–ç•¥ lr=0.01 ç‚ºä¾‹
    optimizer = optim.Adam(defense_model.parameters(), lr=0.05, weight_decay=1e-4, eps=1e-4)
    dataset_MAE = torch.utils.data.TensorDataset(Htrain_tensor)
    dataloader_MAE = torch.utils.data.DataLoader(dataset_MAE, batch_size=batch_size_MAE, shuffle=True)

    # åˆå§‹åŒ– VerticalFLMAELossï¼ˆä¾†è‡ª defense_model_structure.pyï¼‰
    # vertical_loss_fn = dms.VerticalFLMAELoss()
    for e in range(epochs_MAE):
        epoch_loss = 0.0
        for batch in dataloader_MAE:
            batch_x = batch[0].to(device).detach()  # detach ä»¥åˆ‡æ–·ä¹‹å‰çš„åœ–
            mae_input = batch_x.clone()  # clone å‡ºä¸€ä»½ç¨ç«‹çš„å¼µé‡
            MAE_output = defense_model(mae_input)
            MAE_loss = defense_model.loss(MAE_output)
            # loss = vertical_loss_fn(model_output)
            optimizer.zero_grad()
            MAE_loss.backward()  # é€™è£¡ä¸éœ€è¦ retain_graph=True
            optimizer.step()
            epoch_loss += MAE_loss.item()
        print("[SHAP_MAE Offline] Epoch {}/{}: loss = {:.4f}".format(e+1, epochs_MAE, epoch_loss/len(dataloader_MAE)))
    defense_model.train_mode = False
    # defense_model.set_adaptive_threshold(Htrain_tensor)
    return defense_model

def train_SHAP_MAE_D_offline(defense_model,
                             concat_embeddings, labels, args):
    assert concat_embeddings.size(0) == labels.size(0), \
        f"N_emb={concat_embeddings.size(0)}  N_lab={labels.size(0)} é•·åº¦ä¸ç¬¦"
    labels = labels.to(torch.long)
    """å…ˆè·‘èˆŠ SHAP_MAE é›¢ç·šè¨“ç·´ï¼Œå†è£œ cosine çµ±è¨ˆ"""
    defense_model = train_SHAP_MAE_offline(defense_model, concat_embeddings, args)
    # compute_cos_robust_stats(defense_model, concat_embeddings, labels)
    compute_cos_robust_stats(defense_model,
                             concat_embeddings,
                             labels.to(concat_embeddings.device))
    return defense_model

def compute_center_loss(embed, labels, center_layer, parts, d):
    """
    embed : (B, P*d)   -- model._cached_embed
    labels: (B,)       -- y_in (GPU tensor)
    center_layer :     -- model.center_loss (nn.Module)
    parts  : int       -- args.P
    d      : int       -- args.bottomModel_output_dim
    return : scalar tensor
    """
    # ä½¿ç”¨ tensor 0 ä»¥ç¶­æŒç›¸å®¹è£ç½®èˆ‡ dtype
    loss_sum = torch.zeros([], device=embed.device)
    for p in range(parts):
        # ä¸è¦ detachï¼›æ¢¯åº¦è¦å›åˆ°åº•å±¤ bottom-models
        feats_p = embed[:, p*d:(p+1)*d]          # (B, d)
        loss_sum = loss_sum + center_layer(feats_p, labels)
    return loss_sum / parts

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    # è¼¸å…¥åƒæ•¸è¨­å®šï¼ˆè«‹ä¾å¯¦éš›éœ€æ±‚èª¿æ•´ï¼‰
    parser.add_argument('--D', type=str, required=True, help='Dataset name, e.g., CIFAR10',choices=["CIFAR10","CINIC10","BM","NUSWIDE"])
    parser.add_argument('--P', type=int, default=2, help='Number of participants')
    parser.add_argument('--epochs', type=int, default=50, help='Training epochs for VFL')
    parser.add_argument('--AM', type=str, default="VILLIAN", help='Attack method')
    parser.add_argument('--AN', type=int, default=1, help='Number of attackers')
    parser.add_argument('--AL', type=str, default="C", help='Attacker location setting')
    parser.add_argument('--DM', type=str, default="SHAP_MAE", help='Defense method', choices=['NONE','VFLIP','SHAP_MAE','ABL','NEURAL_CLEANSE','CoPur',"VFLMonitor"])
    parser.add_argument('--act', type=bool, default=True, help='æ˜¯å¦æœ‰ active party')
    # parser.add_argument('--batch_size', type=int, default=500, help='Batch size')
    parser.add_argument('--bottomModel_output_dim', type=int, default=50, help='Bottom model output dimension')
    # parser.add_argument('--num_class', type=int, default=10, help='Number of classes')
    parser.add_argument('--label', type=int, required=False, default=0, help='the attack target class')
    # å…¶ä»–åƒæ•¸ä¾éœ€æ±‚åŠ å…¥
    
    # --- SHAP_MAE è¶…åƒæ•¸ -------------------------------------------------
    parser.add_argument('--shap_alpha', type=float, default=None,
                    help='alpha used in Shapley-threshold calculation; '
                         'if None, fall back to the dataset-specific rule')

    # --- ABL hyper-parameters ----------------------------------
    parser.add_argument('--abl_gamma', type=float, default=0.5)
    parser.add_argument('--abl_percent', type=float, default=0.01)
    parser.add_argument('--abl_turn_epoch', type=int, default=20)
    
    # --- CoPur hyper-parameters ---------------------------------
    parser.add_argument('--copur_delta', type=float, default=0.0,
                        help='Î´ in Eq.(6): AE reconstruction budget; 0 â†’ auto (95-th percentile)')
    parser.add_argument('--copur_tau',   type=float, default=1.0,
                        help='Ï„ in Eq.(6): trade-off between ||hâˆ’l||â‚‚,â‚ and manifold error')
    parser.add_argument('--copur_iter',  type=int,   default=200,
                        help='Number of gradient steps when solving Eq.(6) during inference')
    
    # --- SHAP_MAE_D hyper-parameters ---------------------------------
    parser.add_argument('--mae_dim', type=int, default=256, help='SHAPMAE_D hidden dim')
    parser.add_argument('--mae_epochs', type=int, default=20,help='SHAPMAE_D offline training epochs')
    parser.add_argument('--k_cos', type=float, required=False, default=0.01, help='the weight of cosine_mad')
    
    # --- VFLMonitor settings ------
    # parser.add_argument('--center_loss_weight', type=float, default=0.0) #0.003
    parser.add_argument('--swd_proj', type=int, default=128)
    parser.add_argument('--tie_rand_seed', type=int, default=42)
    
    # poison_ratio setting
    parser.add_argument('--poison_ratio', type=float, default=None,
                    help='è‹¥è¨­å®š (0~1)ï¼Œä¾æ¯”ä¾‹æŠ½å– steal_idï¼›æœªè¨­å®šå‰‡æ²¿ç”¨åŸæœ¬é‚è¼¯')
    
    #BM ç‰¹å¾µåˆ†é…
    parser.add_argument('--bm_alloc', type=str, default='active_gets_dur',
                    choices=['equal','dur_to_att','active_gets_dur'],
                    help='BM ç‰¹å¾µåˆ†é…ç­–ç•¥')
    
    
    args = parser.parse_args()
    args.batch_size = return_batch_size(args)
    args.start_poison_epoch = Attack.get_start_poison_epoch(args)
    check_poison_ratio(args)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    args.device = device
    if args.DM =="VFLMonitor":
        args.center_loss_weight = 0.003
    # args.batch_size = 500
    print(f"ç’°å¢ƒä¸­æ˜¯å¦æœ‰ active party: {args.act}")
    print(f"ä½¿ç”¨ {args.D} è³‡æ–™é›†") 
    data.set_party_num(args.P)
    data.set_bm_alloc(args.bm_alloc)
    # å–å¾—è³‡æ–™ï¼ˆdata.get_data ç¯„ä¾‹ï¼‰
    (trainloader, testloader, input_size, num_class, trn_x, trn_y, 
     dl_train_set, poisoned_trainloader, val_x, val_y, dl_val_set, poisoned_testloader) = data.get_data(args.D, args.batch_size)
    args.num_class = num_class

    # åŸ·è¡Œ poisoning_trainï¼ˆåŒ…å«ä¸»æ¨¡å‹è¨“ç·´åŠé›¢ç·š MAE è¨“ç·´ï¼‰
    model, attacker_list, attacker_target_vec_dict, active_party, passive_party, defense_output, train_shap_dict = poisoning_train(
        dl_train_set, poisoned_trainloader, trn_x, trn_y, args
    )
    import eval
    # æ¸¬è©¦éƒ¨åˆ†
    args.attacker_list = attacker_list
    args.active_party = active_party
    args.passive_party = passive_party
    clean_test_shap_dict= eval.eval_clean_train(model, testloader, args, defense_output)
    if (args.AM != "None"): 
        poisoned_test_shap_dict = eval.eval_poisoned_train(model, dl_val_set, poisoned_testloader, attacker_target_vec_dict, attacker_list, args, defense_output)
